# Modu-Messenger Backend

## Project Info

### Backend

- [Spring Boot](https://spring.io/projects/spring-boot)
- [Spring Cloud](https://spring.io/projects/spring-cloud)
- [Spring Cloud Api Gateway](https://spring.io/projects/spring-cloud-gateway)
- [Spring Cloud Eureka](https://cloud.spring.io/spring-cloud-netflix/reference/html/)
- [Spring Cloud Config](https://docs.spring.io/spring-cloud-config/docs/current/reference/html/)
- [Spring Cloud OpenFeign](https://cloud.spring.io/spring-cloud-openfeign/reference/html)
- [Spring Cloud Bus](https://docs.spring.io/spring-cloud-bus/docs/current/reference/html/)
- [Spring Cloud Sleuth](https://cloud.spring.io/spring-cloud-sleuth/reference/html/)
- [Spring Security](https://spring.io/projects/spring-security)
- [Spring Data JPA](https://spring.io/projects/spring-data-jpa)
- [Spring Querydsl](http://querydsl.com/)
- [Spring WebSocket](https://spring.io/guides/gs/messaging-stomp-websocket/)
- [Spring AMQP](https://spring.io/projects/spring-amqp)
- [Springfox Swagger UI](http://springfox.github.io/springfox/docs/current/)
- [JSON Web Tokens](https://jwt.io/)

### Database

- [MySQL](https://www.mysql.com/)
- [Redis](https://redis.io/)

### Infra Structure

- [Amazon Web Services](https://aws.amazon.com/)
- [Docker](https://www.docker.com/)
- [k8s](https://kubernetes.io/ko/)
- [minio](https://min.io/)
- [Rabbit MQ](https://www.rabbitmq.com/)
- [Zipkin](https://zipkin.io/)

## Project Architecture

### MSA (Micro Service Architecture)
초기의 모두의 메시저 서비스는 모놀리식 구조를 가지고 있었습니다.  
모놀리식 구조의 경우 어떠한 장애로 인해 서비스가 다운되는 경우 전체 서비스에 대한 이용이 불가능 하다는 단점이 있었습니다.  
예를 들어 유저에 관련된 요청 로직에 장애가 생겨 서버가 다운되는 경우 채팅과 같은 기능 이용을 불가능 했습니다.  
서비스를 만들면서 만일 유저 관련 서비스에 장애가 생기더라도 채팅을 수행할 수 있도록 하고 싶었습니다.  
또한 장애에 대한 파악이 용이하과, 신속한 대응이 가능하도록 하여 사용자에게 좋은 경험을 남기는 서비스를 만들고 싶었습니다.  
그래서 바로 마이크로 서비스 아키텍쳐 구조를 선택하게 되었습니다.   

## Project Structure

### Spring Cloud API Gateway
모두의 메신저 서비스는 jwt 토큰을 통해 사용자 인증을 수행하고 있습니다.  
리소스에 접근은 사전에 발급된 토큰이 유효한 경우에만 접근이 가능하도록 합니다.  
또한 토큰의 경우 유효 기간이 존재해 만료된 토큰은 재발급을 통해 재인증을 수행하게 됩니다.  
모놀리식 구조에서는 한번의 인증으로 리소스에 접근이 가능했습니다.  
하지만 MSA 구조에서는 다수의 서비스에 접근 하기 위해서는 다수의 인증을 수행해야 했습니다.  
각각의 서비스들은 모두 동일한 인증 기능을 가지고 있어야 했고 중복되는 요청으로 트래픽이 증가할 것으로 보였습니다.  

이러한 문제를 해결하고자, spring api-gateway를 사용하였습니다.  
spring api-gateway를 서비스의 가장 앞단에서 인증에 대한 기능을 수행합니다.  
인증이 완료된 이후에는 요청을 각각의 서비스로 로드 밸런싱 하는 기능을 가지고 있습니다.  
인증되지 않은 요청으 gateway에서 처리 하여 클라이언트에서 서비스의 진입점을 하나의 통로를 지나도록 하였습니다.  
외부로 노출되는 포트가 하나이므로 클라이언트 입장에서는 gateway에 대한 정보만을 가지고 리소스 접근이 가능합니다.  
gateway 뒤에 위치한 서비스에 대한 정보를 알고 있다고 하더라도 외부로 노출되지 않아 정상 응답을 받을 수 없습니다.  

서비스를 운영하다보면 인증을 위한 데이터가 없는 상태에서 발생하는 요청도 존재하게 됩니다.  
예를 들어 회원 가입과 같은 요청은 인증을 위한 데이터가 존재하지 않는 상태에서 이뤄지는 요청입니다.  
인증이 필요하지 않은 요청의 경우에는 api-gateway에서 인증 필터를 수행하지 않아야 했습니다.  
이러한 인증이 불필요한 요청등은 인증을 수행하지 않고 요청을 수행하도록 필터 설정이 가능했습니다.  
그래서 일부 요청의 경우 인증 수행을 하지 않고 정상적으로 서비스를 이용할 수 있도록 하였습니다.  

### Spring Cloud Eureka
모놀리식 구조에서는 하나의 서비스에서 사용자 인증 부터 리소스 접근에 대한 모든 요청을 처리 하였습니다.  
예를 들어, 회원 가입-로그인-서비스 이용이 하나의 서버에서 이뤄지다 보니 구조가 단순하였습니다.  
하지만 MSA 구조에서는 하나의 요청을 처리하기 위해 내부의 다른 서비스로의 접근이 이뤄지는 경우가 많았습니다.  
또한 클라이언트가 하나의 서비스에만 접근 하는것이 아닌 다수의 서비스에 서로 다른 요청을 전송하고 있었습니다.  

클라이언트 또는 내부에서 서로 다른 서비스에 접근 하기 위해서는 접근 하려는 서비스에 대한 정보를 필요로 하였습니다.  
하지만 각각의 서비스가 서로 다른 포트를 점유하고 있고, 서로 각각의 서비스의 포트를 모두 알고 있어야 했습니다.  
또한 특정 서비스의 부하가 생기는 경우 해결하기 위해 동일한 기능을 하는 서비스를 일시적으로 다수 띄우는 경우도 존재합니다.  
이런 경우 변경되는 서비스에 대한 정보를 모두 가지고 있을 수 없는데 기존의 방법으로는 이를 해결할수 없었습니다.   

이러한 문제를 해결하고자, spring eureka를 사용하였습니다.  
각각의 서비스는 discovery client 로써 eureka server로 자신의 정보를 주기적으로 요청을 전송합니다.  
이런경우 서비스를 운영하면서 동적으로 변경되는 서비스의 구성을 모두 알고 있을 필요가 없었습니다.  
모든 정보는 eureka server가 가지고 있으므로, eureka 서버에 대한 정보만을 가지고 있으면 되기 때문입니다.  

eureka server는 등록된 서비스로를 알고 있다해도 부하 부산을 위해서는 로드 밸런싱 기능을 필요로 했습니다.  
로드 밸런싱 기능은 spring api-gateway를 통해 처리하도록 하였습니다.  
클라이언트는 서비스의 가장 앞단에 있는 spring api-gateway로 요청을 전송합니다.  
gateway는 받은 요청의 서비스에 대한 정보를 eureka로 부터 전달 받고 요청에 해당되는 서비스로 요청을 전송하게 됩니다.  

### Spring Cloud Open Feign
MSA 구조에서는 서비스 끼리의 통신이 빈번하게 발생합니다.  
하나의 요청을 수행하기 위해서 내부 통신을 여러번 수행하게되는 경우가 많습니다.  
내부끼리 통신을 하기 위해서는 요청을 전송하는 방법이 필요합니다. spring 에서는 RestTemplte과 같은 기능을 제공하고 있습니다.  
안드로이드 개발시에 사용하였던 http client 라이브러리인 Retrofit는 인터페이스와 어노테이션을 통해 사용이 가능했습니다.  
그래서 반복적으로 입력해야하는 코드가 많은 RestTemplate이 아닌 OpenFeign 을 사용하여 개발하였습니다.  

OpenFeign을 기본 기능도 정말 훌륭하지만, 에러처리와 로깅에 대해서는 조금 수정을 해줘야 했습니다.  
MSA 내부에서 요청에 실패한 경우에 대한 에러 처리가 필요하여, Error decoder를 통해 에러를 커스터 마이징 하였습니다.  
서비스간에 발생하는 서비스에 대한 정보를 알고있어야 하는건 아닌가? 했는데, eureka client로 등록해었기 때문에,  
eureka 서버에 등록된 서비스들의 이름을 통해 서비스 끼리의 통신이 가능했습니다.  

### Spring Cloude Sleuth
MSA 구조로 변경하면서 크게 고민하였던 문제가 있었습니다. 장애가 발생하는 구간을 어떻게 모니터링 할수 있을까? 였습니다.  
MSA 구조에서는 내부 끼리의 통신이 많이 이뤄지기 때문에 장애 구간의 파악이 쉽지 않았습니다.  
특정 서비스에서 부하가 발생한다면 요청 자체에 대한 응답시간이 길어지게 되는데, 부하가 발생하는 구간을 빠르게 쉽게 알고 싶었습니다.  
요청의 시작 부터 끝까지 장애 구간을 명확히 파악하고 대응을 할수 있었으면 좋겠다는 생각을 했습니다.  

그러던 중 찾게 된게 바로 zipkin 이었습니다. zipkin을 이용하면 MSA 환경에서 트래픽 추적이 가능했습니다.  
spring 에서는 주로 사용하는 zipkin 연동 라이브러리인 sleuth 라이브러리를 사용하였습니다.  
특히나 모두의 메신저 서비스는 채팅을 위해서 연결형 프로토콜인 웹소켓을 이용하고 있습니다.  
또한 메시지 브로커로는 rabbitmq를 사용하고 있는데, zipkin과 sleuth를 이용해 트래픽에 대한 흐름을 확인 할수 있었습니다.  

### Spring Security
모두의 메신저는 oauth를 이용한 소셜 로그인 기능을 지원하고, jwt토큰을 통해 사용자 인증을 진행하고 있습니다.  

### Spring Cloud Config & Cloud Bus
MSA 구조에서는 다양한 서비스들이 존재하게 됩니다. 이들은 서로 다른 데이터 베이스에 데이터를 저장하기도 합니다.  
하자만 데이터의 동기화 같은 문제로 인해 다수의 서비스에서 하나의 데이터 베이스에 접근하는 경우가 발생합니다.  
설정 정보를 공통적으로 사용하게 되면 동일한 내용을 각각의 서비스에서 가지고 있게 됩니다.  
이런 경우 중복되는 설정 정보의 문제도 있지만 설정 정보가 변경된다면, 설정을 가지고 있는 모든 서비스를 변경 해야합니다.  
또한 변경후 서비스의 on/off로 인해 정상적으로 서비스를 제공할수 없게 됩니다.  

만일 설정 정보를 공유하고, 변경되는 정보를 업데이트 할수 있는 방법이 없나 찾아보다가 spring cloud config를 알게되었습니다.  
설정 정보를 하나의 서버에서 관리하고, 각각의 서비스는 config-server와의 연동을 통해 설정 정보를 공유하개 됩니다.  
서비스 운영중 설정 정보가 변경된다고 하더라도 spring bus를 이용해 설정 정보를 broad-cast 하게 변경 할수 있습니다.  
spring bus 는 설정 정보 변경시 rabbit mq를 통해 변경된 설정 정보를 전달합니다.  
변경 정보를 전달 하기 위해서 config-server에 busrefresh 요청을 전송하고, 변경된 내용은 각 서비스에 전달되고 반영됩니다.  

### rabbit-mq 를 이용한 메시징 처리

## Dev Diary

### 로그인과 사용자 인증 개선 과정

모두의 메신저는 oauth를 통한 소셜 로그인 기능을 지원하고 jwt 토큰을 통해 사용자 인증을 하고 있습니다.  
로그인 하는 경우 access 토큰과 refresh 토큰을 발급하여 클라이언트에게 전달하며, refresh 토큰만 redis에 저장합니다.  
모든 인증은 Header에 토큰을 설정하여 요청 하도록 하였습니다.  
이후 토큰이 만료되면 클라이언트는 reissue 요청을 통해 토큰을 재발급 받고, 백엔드는 토큰을 갱신합니다.  
access 토큰의 만료 기간을 짧게 잡아 만일 토큰이 탈취를 당한다고 하더라도 정해진 시간 내에만 리소스 접근이 가능하도록 하였습니다.  
로그인은 spring security 를 이용하여 인증을 하도록 하였습니다.  

### 채팅 서버
채팅 기능은 메인 기능 이므로, 가장 많은 부하가 걸린다. 이를 개선하기 위해 여러 컨테이너를 띄워 문제를 해결하려고 했다.  
근데 http와 같은 비연결형 통신이 아닌 연결형 통신인 웹 소켓을 사용하고 있어 고민을 하게되었다.  
단체 채팅방의 경우, 혹은 1대1 채팅방이라고 할지라도 만약에 다른 컨테이너에 연결된 유저에게 어떻게 채팅을 전송할까 하는 문제였다.  
여러가지 방법이 있곘지만, Redis의 Pub/Sub을 통해 해결하였다.  
모든 채팅 서버는 구동할때 새로운 채팅이 있음을 전달 받을 채널을 Subsribe 한다.  
클라이언트가 채팅을 전송하면, 이를 데이터 베이스에 새로운 채팅 메시지를 저장한다.  
그리고 새로운 채팅 메세지의 정보와 채팅방 정보를 Subscribe 중인 다른 채팅 서버에게 Publish 한다.  
Subscribe 중인 채널로 부터 다른 채팅 서버에서 보낸 데이터가 전달 되면,  
데이터 베이스에서 전송 받은 채팅 방의 정보와 채팅 정보를 조회한다.  
조회한 채팅 메세지를 채팅방에 속해있는 유저들에게 전송한다. 이때 연결되어있지 않은 유저에게는 전송하지 않는다.  

### 채팅방 - 유저 (다대다 양방향 연관관계)
초기 설계는 다대다 연관관계 (@ManyToMany)를 사용하여 양방향 관계로 설계   
그러나 프로젝트 요구사항을 채팅방 - 유저간의 다대다 연관관계를 테이블 두개의 조인만으로 해결할수 없을 것으로 보임   
예를 들어 채팅방 - 유저 연관관계 사이에 데이터가 추가로 필요한 경우 두개의 테이블의 맵핑이 불가능함   
연관 테이블은 채팅방멤버(임시 이름)테이블을 두어 다대다 연관관계를 1대다의 연관관계로 풀어나감  
채팅방은 - 채팅방 멤버 - 유저 (1-N, N-1, 모두 양방향)  
채팅방, 유저 테이블은 본 프로젝트의 핵심이되는 테이블로 부하가 가장 많이 생기는 테이블   
따라서 쿼리가 추가로 발생하는 상황 (N+1 문제)를 최대한 피하고자 초기설게에서 연관관계를 수정함   

### 채팅 페이징 처리 (No offset 페이징 처리)
초기 설계는 페이징 처리를 통해서 오프셋과 사이즈를 통해서 처리를 하려고 했다.  
하지만 채팅의 경우 1페이지에서 3페이지로 건너뛰는 경우가 없다.  
스크롤하다가 스크롤이 끝에 도달한 경우에만 이전 데이터를 가져오면 된다.  
그래서 정해진 사이즈와 마지막으로 표시된 채팅의 정보만으로 조회가 가능하다.  

그런데 마지막으로 표시된 채팅이 없는 경우가 존재한다. 어떤 경우일까?  
하나는 채팅방에 처음 입장하는 순간이다. 마지막으로 표시된 채팅이 없기 때문이다.  
이경우에는 채팅방의 채팅을 조회한뒤 마지막에서부터 정해진 크기만큼의 데이터만을 조회한다.  
두번째는 채팅방을 처음 생성한 경우이다. 처음 생성하면 이전 채팅 내역이 존재하지 않는다.  
따라서 채팅방의 채팅 갯수를 가져오는 api를 통해 채팅 조회 요청을 전송할지를 결정해줘야 한다.  

### 푸쉬 알림 (파이어 베이스 클라우드 메시징)
파이버 베이스 클라우드 메시징 (fcm) 연동을 통해, 푸쉬 알림을 전송하는 기능을 추가하였음.  
전송 방식은 토큰을 기준으로 전송하는 방법과 토픽을 통해 전송하는 방식 두개가 있다.  
초기 설계는 클라이언트가 토큰을 발급 받고 백엔드로 전송하면, 백엔드에서 이를 관리한다.  
그리고 메시지가 전송될때마다 채팅방별로 저장된 유저의 토큰을 가지고 와서, 파이어 베이스로 전송하는 것이었다.  
그러나 채팅방 멤버의 수가 많아지면 많아질수록 멤버의 수만큼 loop를 돌면서 전송해야한다.  

그래서 topic을 통해 전송 하는 방법으로 설계를 변경하였다. (토픽은 채팅방 id을 가지고 사용한다)  
topic을 통해 푸시 알림을 전송하면 토큰 발급 후 백엔드로 전송할 필요가 없다.  
(전체 알림 등을 보낼때는 필요하다. 그래서 추후에 전체 알림을 위해 전송 로직은 그대로 남겨 두었다.)  
클라이너트는 구독하려는 토픽만 구독하고, 백엔드는 메시지 이벤트 발생시에 topic을 가지고 파이어 베이스로 전송한다.  
그러면 파이어 베이스에서 topic 을 구독중인 기기로 푸시 알림을 전송한다.   
topic을 이용하면 채팅방 내에 속한 멤버들에게 개별 적으로 토큰을 가지고 전송 전송할 필요가 없어진다.  

만일 전체 유저에게 푸쉬 알림을 전송해야하는 경우에는 일반 메시지 타입이 아닌 멀티 캐스트 메시지를 사용한다.  
멀티 캐스트 메시지의 경우 보낼수있는 수신자가 최대 1000명이다. 그래서 적당한 숫자로 나눠서 전송해준다.  
나는 1000개의 절반인 500개로 설정하였다. 전체 유저에게 푸시 알림을 전송하는 경우 500개 단위로 나눠서 전송한다.  
